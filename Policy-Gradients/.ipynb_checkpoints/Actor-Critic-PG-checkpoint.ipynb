{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our policy network\n",
    "class PolicyNetwork:\n",
    "    '''\n",
    "    Given a state, it outputs the action probabilities\n",
    "    '''\n",
    "    def __init__(self,state_size,action_size,learning_rate=1e-5,name='PolicyNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # Define placeholders\n",
    "        with tf.variable_scope(self.name,reuse=tf.AUTO_REUSE):\n",
    "            with tf.name_scope(self.name + '_placeholders'):\n",
    "                self.advantage = tf.placeholder(shape=[None],name='advantage',dtype=tf.float32)\n",
    "                self.actions_ = tf.placeholder(shape=[None,self.action_size],name='actions_',dtype=tf.float32)\n",
    "                self.state = tf.placeholder(shape=[None,self.state_size],name='state',dtype=tf.float32)\n",
    "            \n",
    "            # Build the network\n",
    "            # We just have a 3 full connected layers...That's it!\n",
    "#             self.fc_1 = tf.layers.dense(self.state,128,name='fc_1'\\\n",
    "#                                         ,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "#             self.fc_2 = tf.layers.dense(self.fc_1,256,name='fc_2'\n",
    "#                                        ,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.output = tf.layers.dense(self.state,action_size,name='output',activation=None\n",
    "                                       ,kernel_initializer=tf.zeros_initializer())\n",
    "            \n",
    "            # Action probability distribution\n",
    "            self.actions = tf.nn.sigmoid(self.output)\n",
    "            \n",
    "            # Take this as a supervised learning problem, we have the labels as the actions\n",
    "            self.neg_log_probs = tf.nn.softmax_cross_entropy_with_logits(logits=self.output,labels=self.actions_)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.neg_log_probs*self.advantage)\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "    def predict(self,sess,state):\n",
    "        '''\n",
    "        Predict the action probabilities\n",
    "        \n",
    "        state : the numpy array of the state passed\n",
    "        '''\n",
    "        action_probs = sess.run(self.actions,feed_dict={self.state:state})\n",
    "        return action_probs\n",
    "    \n",
    "    def update(self,sess,state,actions,advantage):\n",
    "        '''\n",
    "        Make the optimizer update\n",
    "        state : the state for which we make the update\n",
    "        '''\n",
    "        _,loss = sess.run([self.optimizer,self.loss],feed_dict={self.state:state,\\\n",
    "                                                                self.actions_:actions,\\\n",
    "                                                                self.advantage:advantage})\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimatorNetwork:\n",
    "    '''\n",
    "    This computes the generalized advantage approximated by the TD error on the State-Value function\n",
    "    TD-error = r + gamma*V(s_t+1) - V(s_t)\n",
    "    '''\n",
    "    def __init__(self,state_size,learning_rate=1e-5,gamma=0.99,name = 'AdvantageNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.name = name\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "                # Define placeholders\n",
    "                with tf.name_scope('Placeholders'):\n",
    "                    self.state = tf.placeholder(shape=[None,self.state_size],name='state',dtype=tf.float32)\n",
    "                    self.target = tf.placeholder(shape=[None],name='target',dtype=tf.float32)\n",
    "                \n",
    "                # Build the network\n",
    "#                 self.fc_1 = tf.layers.dense(self.state,128,name='fc_1',\\\n",
    "#                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                self.output = tf.layers.dense(self.state,1,name='output',\\\n",
    "                                             activation=None,kernel_initializer=tf.zeros_initializer())\n",
    "                \n",
    "                # output is the value estimate\n",
    "                self.loss = tf.square(self.output - self.target)\n",
    "                \n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "    \n",
    "    def predict(self,sess,state):\n",
    "        '''\n",
    "        Predict the value estimate\n",
    "        \n",
    "        state : the numpy array of the state passed\n",
    "        '''\n",
    "        value_estimate = sess.run(self.output,feed_dict={self.state:state})\n",
    "        return value_estimate\n",
    "    \n",
    "    def update(self,sess,state,target):\n",
    "        '''\n",
    "        Make the optimizer update\n",
    "        state : the state for which we make the update\n",
    "        Update made : V(s) = r + gamma*V(s')\n",
    "        '''\n",
    "        _,loss = sess.run([self.optimizer,self.loss],feed_dict={self.state:state,self.target:target})\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork:\n",
    "    def __init__(self,env,state_size,action_size,actor_lr=1e-5,critic_lr=1e-5,\\\n",
    "                 gamma=0.99,num_episodes=1000,episode_length=1000,\\\n",
    "                 render=False,name='ACNetwork'):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.name = name\n",
    "        self.num_episodes = num_episodes\n",
    "        self.episode_length = episode_length\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.gamma = gamma\n",
    "        self.render = render\n",
    "        \n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        '''\n",
    "        Actor Network is the Policy Network\n",
    "        Critic Network is the Value Estimator Network\n",
    "        '''\n",
    "        tf.reset_default_graph()\n",
    "        self.actor_network = PolicyNetwork(self.state_size,self.action_size,learning_rate=self.actor_lr,name='ActorNetwork')\n",
    "        self.critic_network = ValueEstimatorNetwork(self.state_size,learning_rate=self.critic_lr,name='CriticNetwork')\n",
    "    \n",
    "    def train(self):\n",
    "        '''\n",
    "        Train the actor critic network\n",
    "        '''\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "        \n",
    "        all_rewards = []\n",
    "        \n",
    "        for episode in range(self.num_episodes):\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # Episode Statistics\n",
    "            total_reward = 0.0 \n",
    "            \n",
    "            while True:\n",
    "                if self.render is True:\n",
    "                    self.env.render()\n",
    "\n",
    "                # Choose an action\n",
    "                action_probs = np.array(self.actor_network.predict(self.sess,state.reshape(1,self.state_size))).reshape(self.action_size)\n",
    "\n",
    "                # Apparently numpy gives an error of probabilites not summing upto one otherwise\n",
    "                action_probs/=sum(action_probs)\n",
    "#                 print(action_probs)\n",
    "                action = np.random.choice(np.arange(len(action_probs)),p=action_probs)\n",
    "                \n",
    "#                 print(action)\n",
    "                actions_one_hot = np.zeros((1,self.action_size))\n",
    "                actions_one_hot[0,action] = 1.0\n",
    "                \n",
    "                # Take a step in the environment\n",
    "                next_state,reward,done,_ = self.env.step(action)\n",
    "                \n",
    "                # Compute The TD-Error to be used as the Advantage\n",
    "                V_s = np.array(self.critic_network.predict(self.sess,state.reshape(1,self.state_size)).reshape(1))\n",
    "                V_s_next = np.array(self.critic_network.predict(self.sess,next_state.reshape(1,self.state_size)).reshape(1))\n",
    "                \n",
    "                V_target = reward + self.gamma*V_s_next\n",
    "                TD_error = reward + self.gamma*V_s_next - V_s\n",
    "                                \n",
    "                # Update the critic network\n",
    "                self.critic_network.update(self.sess,state.reshape(1,self.state_size),V_target)\n",
    "                \n",
    "                # Update the actor network\n",
    "                self.actor_network.update(self.sess,state.reshape(1,self.state_size),actions_one_hot,TD_error)\n",
    "       \n",
    "                # Update the statistics \n",
    "                total_reward+=reward\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    all_rewards.append(total_reward)\n",
    "                    break\n",
    "            \n",
    "            mean_reward = np.sum(all_rewards)/(1.0*(episode + 1))\n",
    "            print('Episode : {}\\nTotal Reward : {}\\nMean Reward : {}'.format(episode,total_reward,mean_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# For the cartpole environment\n",
    "state_size = 4\n",
    "action_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorCriticNet = ActorCriticNetwork(env,state_size,action_size,actor_lr=0.01,critic_lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0\n",
      "Total Reward : 20.0\n",
      "Mean Reward : 20.0\n",
      "Episode : 1\n",
      "Total Reward : 22.0\n",
      "Mean Reward : 21.0\n",
      "Episode : 2\n",
      "Total Reward : 12.0\n",
      "Mean Reward : 18.0\n",
      "Episode : 3\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 16.0\n",
      "Episode : 4\n",
      "Total Reward : 19.0\n",
      "Mean Reward : 16.6\n",
      "Episode : 5\n",
      "Total Reward : 35.0\n",
      "Mean Reward : 19.666666666666668\n",
      "Episode : 6\n",
      "Total Reward : 22.0\n",
      "Mean Reward : 20.0\n",
      "Episode : 7\n",
      "Total Reward : 15.0\n",
      "Mean Reward : 19.375\n",
      "Episode : 8\n",
      "Total Reward : 12.0\n",
      "Mean Reward : 18.555555555555557\n",
      "Episode : 9\n",
      "Total Reward : 17.0\n",
      "Mean Reward : 18.4\n",
      "Episode : 10\n",
      "Total Reward : 34.0\n",
      "Mean Reward : 19.818181818181817\n",
      "Episode : 11\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 19.0\n",
      "Episode : 12\n",
      "Total Reward : 16.0\n",
      "Mean Reward : 18.76923076923077\n",
      "Episode : 13\n",
      "Total Reward : 14.0\n",
      "Mean Reward : 18.428571428571427\n",
      "Episode : 14\n",
      "Total Reward : 13.0\n",
      "Mean Reward : 18.066666666666666\n",
      "Episode : 15\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 17.625\n",
      "Episode : 16\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 17.11764705882353\n",
      "Episode : 17\n",
      "Total Reward : 12.0\n",
      "Mean Reward : 16.833333333333332\n",
      "Episode : 18\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 16.36842105263158\n",
      "Episode : 19\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 16.1\n",
      "Episode : 20\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 15.761904761904763\n",
      "Episode : 21\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 15.5\n",
      "Episode : 22\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 15.217391304347826\n",
      "Episode : 23\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 14.916666666666666\n",
      "Episode : 24\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 14.72\n",
      "Episode : 25\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 14.461538461538462\n",
      "Episode : 26\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 14.296296296296296\n",
      "Episode : 27\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 14.107142857142858\n",
      "Episode : 28\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 13.89655172413793\n",
      "Episode : 29\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 13.8\n",
      "Episode : 30\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 13.709677419354838\n",
      "Episode : 31\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 13.5625\n",
      "Episode : 32\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 13.454545454545455\n",
      "Episode : 33\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 13.352941176470589\n",
      "Episode : 34\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 13.257142857142858\n",
      "Episode : 35\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 13.166666666666666\n",
      "Episode : 36\n",
      "Total Reward : 12.0\n",
      "Mean Reward : 13.135135135135135\n",
      "Episode : 37\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 13.026315789473685\n",
      "Episode : 38\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 12.923076923076923\n",
      "Episode : 39\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 12.875\n",
      "Episode : 40\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 12.829268292682928\n",
      "Episode : 41\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 12.761904761904763\n",
      "Episode : 42\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 12.720930232558139\n",
      "Episode : 43\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 12.659090909090908\n",
      "Episode : 44\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 12.6\n",
      "Episode : 45\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 12.521739130434783\n",
      "Episode : 46\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 12.446808510638299\n",
      "Episode : 47\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 12.375\n",
      "Episode : 48\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 12.306122448979592\n",
      "Episode : 49\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 12.24\n",
      "Episode : 50\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 12.176470588235293\n",
      "Episode : 51\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 12.134615384615385\n",
      "Episode : 52\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 12.075471698113208\n",
      "Episode : 53\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 12.018518518518519\n",
      "Episode : 54\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.981818181818182\n",
      "Episode : 55\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.946428571428571\n",
      "Episode : 56\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 11.87719298245614\n",
      "Episode : 57\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.827586206896552\n",
      "Episode : 58\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.796610169491526\n",
      "Episode : 59\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.75\n",
      "Episode : 60\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 11.688524590163935\n",
      "Episode : 61\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.661290322580646\n",
      "Episode : 62\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.634920634920634\n",
      "Episode : 63\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 11.578125\n",
      "Episode : 64\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.538461538461538\n",
      "Episode : 65\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.5\n",
      "Episode : 66\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.462686567164178\n",
      "Episode : 67\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.426470588235293\n",
      "Episode : 68\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.391304347826088\n",
      "Episode : 69\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.371428571428572\n",
      "Episode : 70\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.338028169014084\n",
      "Episode : 71\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 11.333333333333334\n",
      "Episode : 72\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 11.32876712328767\n",
      "Episode : 73\n",
      "Total Reward : 14.0\n",
      "Mean Reward : 11.364864864864865\n",
      "Episode : 74\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.333333333333334\n",
      "Episode : 75\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.302631578947368\n",
      "Episode : 76\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.285714285714286\n",
      "Episode : 77\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.256410256410257\n",
      "Episode : 78\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.240506329113924\n",
      "Episode : 79\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.225\n",
      "Episode : 80\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 11.185185185185185\n",
      "Episode : 81\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 11.146341463414634\n",
      "Episode : 82\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.120481927710843\n",
      "Episode : 83\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 11.083333333333334\n",
      "Episode : 84\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.070588235294117\n",
      "Episode : 85\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.046511627906977\n",
      "Episode : 86\n",
      "Total Reward : 13.0\n",
      "Mean Reward : 11.068965517241379\n",
      "Episode : 87\n",
      "Total Reward : 12.0\n",
      "Mean Reward : 11.079545454545455\n",
      "Episode : 88\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 11.07865168539326\n",
      "Episode : 89\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.066666666666666\n",
      "Episode : 90\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.054945054945055\n",
      "Episode : 91\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 11.043478260869565\n",
      "Episode : 92\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 11.021505376344086\n",
      "Episode : 93\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.98936170212766\n",
      "Episode : 94\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.957894736842105\n",
      "Episode : 95\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.947916666666666\n",
      "Episode : 96\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.938144329896907\n",
      "Episode : 97\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.908163265306122\n",
      "Episode : 98\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.88888888888889\n",
      "Episode : 99\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.89\n",
      "Episode : 100\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.871287128712872\n",
      "Episode : 101\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.852941176470589\n",
      "Episode : 102\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.844660194174757\n",
      "Episode : 103\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.836538461538462\n",
      "Episode : 104\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.82857142857143\n",
      "Episode : 105\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.81132075471698\n",
      "Episode : 106\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.80373831775701\n",
      "Episode : 107\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.787037037037036\n",
      "Episode : 108\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.779816513761467\n",
      "Episode : 109\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.772727272727273\n",
      "Episode : 110\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.756756756756756\n",
      "Episode : 111\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.75\n",
      "Episode : 112\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.725663716814159\n",
      "Episode : 113\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.728070175438596\n",
      "Episode : 114\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.721739130434782\n",
      "Episode : 115\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.698275862068966\n",
      "Episode : 116\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.683760683760683\n",
      "Episode : 117\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.677966101694915\n",
      "Episode : 118\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.655462184873949\n",
      "Episode : 119\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.65\n",
      "Episode : 120\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.636363636363637\n",
      "Episode : 121\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.62295081967213\n",
      "Episode : 122\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.617886178861788\n",
      "Episode : 123\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.596774193548388\n",
      "Episode : 124\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.592\n",
      "Episode : 125\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.587301587301587\n",
      "Episode : 126\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.582677165354331\n",
      "Episode : 127\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.5625\n",
      "Episode : 128\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.55813953488372\n",
      "Episode : 129\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.553846153846154\n",
      "Episode : 130\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.534351145038167\n",
      "Episode : 131\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.530303030303031\n",
      "Episode : 132\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.526315789473685\n",
      "Episode : 133\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.507462686567164\n",
      "Episode : 134\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.496296296296297\n",
      "Episode : 135\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.485294117647058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 136\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.481751824817518\n",
      "Episode : 137\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.471014492753623\n",
      "Episode : 138\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.467625899280575\n",
      "Episode : 139\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.471428571428572\n",
      "Episode : 140\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.46808510638298\n",
      "Episode : 141\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.45774647887324\n",
      "Episode : 142\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.461538461538462\n",
      "Episode : 143\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.45138888888889\n",
      "Episode : 144\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.434482758620689\n",
      "Episode : 145\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.431506849315069\n",
      "Episode : 146\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.421768707482993\n",
      "Episode : 147\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.41891891891892\n",
      "Episode : 148\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.409395973154362\n",
      "Episode : 149\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.4\n",
      "Episode : 150\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.403973509933774\n",
      "Episode : 151\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.394736842105264\n",
      "Episode : 152\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.392156862745098\n",
      "Episode : 153\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.383116883116884\n",
      "Episode : 154\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.374193548387098\n",
      "Episode : 155\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.35897435897436\n",
      "Episode : 156\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.356687898089172\n",
      "Episode : 157\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.354430379746836\n",
      "Episode : 158\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.345911949685535\n",
      "Episode : 159\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.3375\n",
      "Episode : 160\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.335403726708075\n",
      "Episode : 161\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.333333333333334\n",
      "Episode : 162\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.337423312883436\n",
      "Episode : 163\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.323170731707316\n",
      "Episode : 164\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.315151515151515\n",
      "Episode : 165\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.301204819277109\n",
      "Episode : 166\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.293413173652695\n",
      "Episode : 167\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.291666666666666\n",
      "Episode : 168\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.284023668639053\n",
      "Episode : 169\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.282352941176471\n",
      "Episode : 170\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.274853801169591\n",
      "Episode : 171\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.273255813953488\n",
      "Episode : 172\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.277456647398845\n",
      "Episode : 173\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.275862068965518\n",
      "Episode : 174\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.274285714285714\n",
      "Episode : 175\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.278409090909092\n",
      "Episode : 176\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.27683615819209\n",
      "Episode : 177\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.269662921348315\n",
      "Episode : 178\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.268156424581006\n",
      "Episode : 179\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.272222222222222\n",
      "Episode : 180\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.2707182320442\n",
      "Episode : 181\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.263736263736265\n",
      "Episode : 182\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.256830601092895\n",
      "Episode : 183\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.25\n",
      "Episode : 184\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.243243243243244\n",
      "Episode : 185\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.236559139784946\n",
      "Episode : 186\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.235294117647058\n",
      "Episode : 187\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.23936170212766\n",
      "Episode : 188\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.238095238095237\n",
      "Episode : 189\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.226315789473684\n",
      "Episode : 190\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.225130890052355\n",
      "Episode : 191\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.223958333333334\n",
      "Episode : 192\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.217616580310882\n",
      "Episode : 193\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.216494845360824\n",
      "Episode : 194\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.21025641025641\n",
      "Episode : 195\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.204081632653061\n",
      "Episode : 196\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.203045685279188\n",
      "Episode : 197\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.202020202020202\n",
      "Episode : 198\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.195979899497488\n",
      "Episode : 199\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.195\n",
      "Episode : 200\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.194029850746269\n",
      "Episode : 201\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.193069306930694\n",
      "Episode : 202\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.187192118226601\n",
      "Episode : 203\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.181372549019608\n",
      "Episode : 204\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.175609756097561\n",
      "Episode : 205\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.174757281553399\n",
      "Episode : 206\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.169082125603865\n",
      "Episode : 207\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.16826923076923\n",
      "Episode : 208\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.17224880382775\n",
      "Episode : 209\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.166666666666666\n",
      "Episode : 210\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.161137440758294\n",
      "Episode : 211\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.160377358490566\n",
      "Episode : 212\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.154929577464788\n",
      "Episode : 213\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.154205607476635\n",
      "Episode : 214\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.153488372093022\n",
      "Episode : 215\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.152777777777779\n",
      "Episode : 216\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.152073732718893\n",
      "Episode : 217\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.15137614678899\n",
      "Episode : 218\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.155251141552512\n",
      "Episode : 219\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.154545454545454\n",
      "Episode : 220\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.153846153846153\n",
      "Episode : 221\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.14864864864865\n",
      "Episode : 222\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.147982062780269\n",
      "Episode : 223\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.147321428571429\n",
      "Episode : 224\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.142222222222221\n",
      "Episode : 225\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.141592920353983\n",
      "Episode : 226\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.136563876651982\n",
      "Episode : 227\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.135964912280702\n",
      "Episode : 228\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.139737991266376\n",
      "Episode : 229\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.130434782608695\n",
      "Episode : 230\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.12987012987013\n",
      "Episode : 231\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.120689655172415\n",
      "Episode : 232\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.11587982832618\n",
      "Episode : 233\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.115384615384615\n",
      "Episode : 234\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.114893617021277\n",
      "Episode : 235\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.114406779661017\n",
      "Episode : 236\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.10548523206751\n",
      "Episode : 237\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.105042016806722\n",
      "Episode : 238\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.104602510460252\n",
      "Episode : 239\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.104166666666666\n",
      "Episode : 240\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.099585062240664\n",
      "Episode : 241\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.09504132231405\n",
      "Episode : 242\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.090534979423868\n",
      "Episode : 243\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.09016393442623\n",
      "Episode : 244\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.085714285714285\n",
      "Episode : 245\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.085365853658537\n",
      "Episode : 246\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.080971659919028\n",
      "Episode : 247\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.076612903225806\n",
      "Episode : 248\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.076305220883533\n",
      "Episode : 249\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.076\n",
      "Episode : 250\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.071713147410359\n",
      "Episode : 251\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.071428571428571\n",
      "Episode : 252\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.071146245059289\n",
      "Episode : 253\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.066929133858268\n",
      "Episode : 254\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.058823529411764\n",
      "Episode : 255\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.05859375\n",
      "Episode : 256\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.054474708171206\n",
      "Episode : 257\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.054263565891473\n",
      "Episode : 258\n",
      "Total Reward : 11.0\n",
      "Mean Reward : 10.057915057915057\n",
      "Episode : 259\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.057692307692308\n",
      "Episode : 260\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.049808429118775\n",
      "Episode : 261\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.045801526717558\n",
      "Episode : 262\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.041825095057034\n",
      "Episode : 263\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.037878787878787\n",
      "Episode : 264\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.037735849056604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 265\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.037593984962406\n",
      "Episode : 266\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.02996254681648\n",
      "Episode : 267\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.029850746268657\n",
      "Episode : 268\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.026022304832713\n",
      "Episode : 269\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 10.018518518518519\n",
      "Episode : 270\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.014760147601477\n",
      "Episode : 271\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.011029411764707\n",
      "Episode : 272\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.007326007326007\n",
      "Episode : 273\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.007299270072993\n",
      "Episode : 274\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 10.007272727272728\n",
      "Episode : 275\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 10.003623188405797\n",
      "Episode : 276\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 9.99638989169675\n",
      "Episode : 277\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 9.992805755395683\n",
      "Episode : 278\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 9.989247311827956\n",
      "Episode : 279\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.989285714285714\n",
      "Episode : 280\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.98932384341637\n",
      "Episode : 281\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.98936170212766\n",
      "Episode : 282\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.989399293286219\n",
      "Episode : 283\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.98943661971831\n",
      "Episode : 284\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.989473684210527\n",
      "Episode : 285\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 9.982517482517483\n",
      "Episode : 286\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.982578397212544\n",
      "Episode : 287\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.98263888888889\n",
      "Episode : 288\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.982698961937716\n",
      "Episode : 289\n",
      "Total Reward : 8.0\n",
      "Mean Reward : 9.975862068965517\n",
      "Episode : 290\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 9.972508591065292\n",
      "Episode : 291\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.972602739726028\n",
      "Episode : 292\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 9.969283276450511\n",
      "Episode : 293\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 9.965986394557824\n",
      "Episode : 294\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.966101694915254\n",
      "Episode : 295\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.966216216216216\n",
      "Episode : 296\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.966329966329967\n",
      "Episode : 297\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.966442953020135\n",
      "Episode : 298\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 9.963210702341136\n",
      "Episode : 299\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 9.96\n",
      "Episode : 300\n",
      "Total Reward : 9.0\n",
      "Mean Reward : 9.956810631229235\n",
      "Episode : 301\n",
      "Total Reward : 10.0\n",
      "Mean Reward : 9.956953642384105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-316-65dae501f634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactorCriticNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-313-ad91633ce365>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;31m# Update the actor network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions_one_hot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTD_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# Update the statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-311-9559a52eb052>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, state, actions, advantage)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mmake\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         '''\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m                                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m                                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvantage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0madvantage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "actorCriticNet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(3,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(1,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.86728292,  0.49781556, -0.26410778])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

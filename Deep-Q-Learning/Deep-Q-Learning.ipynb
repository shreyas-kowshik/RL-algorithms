{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing PER\n",
    "class SumTree:\n",
    "    # Visualised in the form a binray heap\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        self.leaves = size # Number of leaf nodes in the sum tree\n",
    "        self.size = 2*size - 1 # Number of nodes in the sum tree\n",
    "        self.data_pointer = 0\n",
    "        self.tree = np.zeros((self.size))\n",
    "        self.data = np.zeros((self.size),dtype=object) # It stores an experience tuple\n",
    "        \n",
    "    def add(self,priority,data):\n",
    "        tree_index = self.data_pointer + self.leaves - 1 # Refers to the leaf node number indexed by data_pointer\n",
    "\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        self.update(tree_index,priority)\n",
    "        \n",
    "        #Updating to the next index\n",
    "        self.data_pointer+=1\n",
    "        \n",
    "        # Checking for overflow\n",
    "        if self.data_pointer >= self.leaves:\n",
    "            self.data_pointer = 0\n",
    "    \n",
    "    def update(self,tree_index,priority):\n",
    "        # print('In Update---')\n",
    "        # print('Tree Index : ' + str(tree_index))\n",
    "        # print('Priority : ' + str(priority))\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # Update the non-leaf nodes up the tree\n",
    "        parent = ((tree_index + 1)//2) - 1\n",
    "\n",
    "        while parent >= 0:\n",
    "            parent = ((tree_index + 1)//2) - 1\n",
    "            \n",
    "            if parent < 0:\n",
    "                break\n",
    "\n",
    "            left_child = 2*parent + 1\n",
    "            right_child = left_child + 1\n",
    "            self.tree[parent] = self.tree[left_child] + self.tree[right_child]\n",
    "            tree_index = parent\n",
    "\n",
    "    def get_leaf(self,s):\n",
    "        # Return the leaf node such that the cumulative priority till that is <= s\n",
    "        # Values are returned as index,data,priority\n",
    "        # In short this \n",
    "        index = 0 # Start from the root\n",
    "        leaf_index = None\n",
    "        \n",
    "        while True:\n",
    "            left_child = 2*index + 1\n",
    "            right_child = left_child + 1\n",
    "            \n",
    "            if left_child >= self.size: # index is the leaf node\n",
    "                leaf_index = index\n",
    "                break\n",
    "            else:\n",
    "                if self.tree[left_child] >= s:\n",
    "                    index = left_child\n",
    "                else:\n",
    "                    s = s - self.tree[left_child]\n",
    "                    index = right_child\n",
    "        \n",
    "        return leaf_index,self.data[leaf_index - self.leaves + 1],self.tree[leaf_index]\n",
    "    \n",
    "    def get_total_priority(self):\n",
    "        return self.tree[0] # The root value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PER:\n",
    "    def __init__(self,size):\n",
    "        self.size = size\n",
    "        self.tree = SumTree(size)\n",
    "        \n",
    "        # PER constants\n",
    "        self.alpha = 0.4\n",
    "        self.beta = 0.6\n",
    "        self.beta_increment_factor = 0.001\n",
    "        self.epsilon = 0.01\n",
    "        \n",
    "        self.maximum_absolute_error = 1.0 # To clip the TD errors\n",
    "        \n",
    "    def store(self,experience):\n",
    "        # experience : a tuple of (s,a,r,s')\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.leaves:])\n",
    "\n",
    "        if max_priority == 0:\n",
    "            max_priority = 1.0 # Add any random value for now as it will in due course be updated\n",
    "        \n",
    "        # We use maximum  priority for a new experience to ensure that it will be use at least once\n",
    "        # This breaks the bias\n",
    "        self.tree.add(max_priority,experience)\n",
    "    \n",
    "    def sample_minibatch(self,k):\n",
    "        '''\n",
    "        Parameters:\n",
    "        k : mini-batch size\n",
    "        \n",
    "        Returns:\n",
    "        batch_idx : index values of the SumTree of the nodes which were sampled so that their errors can be updated\n",
    "        mini_batch : array of experience tuples in the mini-batch\n",
    "        w : importance sampling weights\n",
    "        '''\n",
    "        \n",
    "        # Samples a minibatch of size k\n",
    "        mini_batch = []\n",
    "        batch_idx = np.zeros((k))\n",
    "        \n",
    "        total_priority = self.tree.get_total_priority()\n",
    "        \n",
    "        # Divide the total probability in bins\n",
    "        num_bins = total_priority/k\n",
    "        \n",
    "        # Importance Sampling Weights\n",
    "        w = np.zeros((k,1))\n",
    "        \n",
    "        P_min = np.min(self.tree.tree[-self.tree.leaves:])/self.tree.get_total_priority()\n",
    "        # print('P_min : ' + str(P_min))\n",
    "        max_w = (1.*k*P_min)**(-self.beta)\n",
    "        # print('Max w : ' + str(max_w))\n",
    "        \n",
    "        for i in range(k):\n",
    "            # Find the bin priority indices\n",
    "            left_index = i*num_bins\n",
    "            right_index = (i+1)*num_bins\n",
    "            s = np.random.uniform(left_index,right_index)\n",
    "            \n",
    "            # Sample a random node with priority in range [i*num_bins,(i+1)*num_bins]\n",
    "            # This samples a node from a probability distribution represented by the priorities\n",
    "            idx,data,priority = self.tree.get_leaf(s)\n",
    "            \n",
    "            mini_batch.append(data)\n",
    "            \n",
    "            # Calculate P(i)\n",
    "            # We assume that the values in the tree are already exponentiated while updation\n",
    "            # i.e the tree leaves store (p(i))**alpha\n",
    "            sampling_probability = priority/self.tree.get_total_priority()\n",
    "            \n",
    "            # Calculate the weights\n",
    "            # print('Priority : ' + str(priority))\n",
    "            # print('Sampling Probability : ' + str(sampling_probability))\n",
    "            # print('Calculated weight : ' + str(np.power(1.*k*sampling_probability,-self.beta)/(max_w)))\n",
    "            w[i,0] = (1.*k*sampling_probability)**(-self.beta)/(max_w)\n",
    "            \n",
    "            # Set the index of the tree leaf\n",
    "            batch_idx[i] = idx\n",
    "        \n",
    "        # Increment beta at each sampling step\n",
    "        self.beta = np.min([1.,self.beta + self.beta_increment_factor])\n",
    "        \n",
    "        return batch_idx.astype(np.int),mini_batch,w\n",
    "    \n",
    "    def update(self,batch_idx,errors):\n",
    "        '''\n",
    "        errors : TD errors\n",
    "        Updates the indices which were sampled with their corrected error values\n",
    "        '''\n",
    "        \n",
    "        absolute_errors = np.abs(errors) + self.epsilon\n",
    "        \n",
    "        # Store (p(i))**alpha in the tree leaves\n",
    "        clipped_errors = np.minimum(absolute_errors,self.maximum_absolute_error) # Clipped in range [0,1]\n",
    "        exponentiated_errors = np.power(clipped_errors,self.alpha)\n",
    "        \n",
    "        # print('In Loop')\n",
    "        for i,delta in zip(batch_idx,exponentiated_errors):\n",
    "            # print(i)\n",
    "            # print(delta)\n",
    "            self.tree.update(i,delta)\n",
    "    \n",
    "    def print_priorities(self):\n",
    "        print('Number of experiences stored : ' + str(len(self.tree.tree[-self.tree.leaves:])))\n",
    "        print('Priorities : ' + str(self.tree.tree[-self.tree.leaves:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PER():\n",
    "    mem_size = 100\n",
    "    nA = 2 # Number of actions\n",
    "\n",
    "    memory = PER(mem_size)\n",
    "\n",
    "    env.reset()\n",
    "    for _ in range(mem_size):\n",
    "        state = env.env.state\n",
    "        action = env.action_space.sample()\n",
    "        next_state,reward,done,_ = env.step(action) # take a random action\n",
    "        if done:\n",
    "            env.reset()\n",
    "        memory.store((state,action,reward,next_state))\n",
    "    \n",
    "    memory.print_priorities()\n",
    "    batch_idx,min_batch,w = memory.sample_minibatch(7)\n",
    "    print(batch_idx)\n",
    "    errors = np.random.randn(5)\n",
    "    print(errors)\n",
    "    print(w)\n",
    "    memory.update(batch_idx,errors)\n",
    "    memory.print_priorities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,state_size,action_size,memory_size=100000,learning_rate=1e-5,name='DQNAgent'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory_size = memory_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(self.name,reuse=tf.AUTO_REUSE):\n",
    "            '''\n",
    "            Our state consists of four stacked frames of the game of shape - (WIDTH,HEIGHT,4)\n",
    "            '''\n",
    "            # For the Importance Sampling Weights\n",
    "            self.weights_placeholder = tf.placeholder(shape=[None,1],name='weights_placeholder',dtype=tf.float32)\n",
    "            self.state = tf.placeholder(shape=[None,*self.state_size],name='state',dtype=tf.float32)\n",
    "            \n",
    "            # actions_ is a one hot vector\n",
    "            self.actions_ = tf.placeholder(shape=[None,self.action_size],name='actions_',dtype=tf.float32) \n",
    "            \n",
    "            # Our target Q value\n",
    "            self.target_Q = tf.placeholder(shape=[None],name='target_Q',dtype=tf.float32)\n",
    "\n",
    "            self.conv1 = tf.nn.elu(tf.layers.conv2d(self.state,filters=32,kernel_size=[4,4],\\\n",
    "                                                     strides=[4,4],name='conv1'),name='elu1')\n",
    "            self.conv2 = tf.nn.elu(tf.layers.conv2d(self.conv1,filters=64,kernel_size=[4,4],\\\n",
    "                                                     strides=[2,2],name='conv2'),name='elu2')\n",
    "            self.conv3 = tf.nn.elu(tf.layers.conv2d(self.conv2,filters=128,kernel_size=[2,2],\\\n",
    "                                                     strides=[2,2],name='conv3'),name='elu3')\n",
    "            self.flat = tf.layers.flatten(self.conv3)\n",
    "            \n",
    "            # Branch out now to V(s) and A(s,a)\n",
    "            self.V_fc = tf.layers.dense(self.flat,128,kernel_initializer=tf.contrib.layers.xavier_initializer()\\\n",
    "                                       ,activation = tf.nn.elu,name='V_fc')\n",
    "            self.V = tf.layers.dense(self.V_fc,1,kernel_initializer=tf.contrib.layers.xavier_initializer()\\\n",
    "                                    ,name='V')\n",
    "            \n",
    "            self.A_fc = tf.layers.dense(self.flat,128,kernel_initializer=tf.contrib.layers.xavier_initializer()\\\n",
    "                                       ,activation = tf.nn.elu,name='A_fc')\n",
    "            self.A = tf.layers.dense(self.A_fc,self.action_size,kernel_initializer=tf.contrib.layers.xavier_initializer()\\\n",
    "                                    ,name='A')\n",
    "            \n",
    "            # This stores the Q values for all possible actions\n",
    "            self.Q_all_a = self.V + (self.A - tf.reduce_mean(self.A,axis=1,keepdims=True))\n",
    "            \n",
    "            # This keeps the values only for the action taken - Q[s,a] in the transition tuple\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.Q_all_a,self.actions_),axis=1)\n",
    "            \n",
    "            self.TD_error = tf.abs(self.target_Q - self.Q)\n",
    "\n",
    "            self.loss = tf.reduce_mean(self.weights_placeholder*tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class StateProcessor(object):\n",
    "    '''\n",
    "    State Processor for the Breakout-v0 environment\n",
    "    '''\n",
    "    def __init__(self,name='Breakout-v0'):\n",
    "        self.name = name\n",
    "        self.env = gym.make(name)\n",
    "        self.stack_size = 4\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.is_new_episode = True\n",
    "        self.state = self.env.reset()        \n",
    "        self.state_stack  =  deque([np.zeros((84,84), dtype=np.int) for i in range(self.stack_size)], maxlen=4)\n",
    "        self.stack_states()\n",
    "    \n",
    "    def stack_states(self):\n",
    "        '''\n",
    "        Stack the 4 previous frames\n",
    "        '''\n",
    "        \n",
    "        if self.is_new_episode is True:\n",
    "            self.state_stack.append(self.process(self.state))\n",
    "            self.state_stack.append(self.process(self.state))\n",
    "            self.state_stack.append(self.process(self.state))\n",
    "            self.state_stack.append(self.process(self.state))\n",
    "            \n",
    "            self.is_new_episode = False\n",
    "        else:\n",
    "            self.state_stack.append(self.process(self.state))           \n",
    "        \n",
    "        self.stacked_frames = np.stack(self.state_stack,axis=2)\n",
    "            \n",
    "    def process(self,state):\n",
    "        '''\n",
    "        Takes as input a (210,160,3) image\n",
    "        Processes it to (84,80,1) image\n",
    "        '''\n",
    "        image = np.array(state,dtype=np.uint8)\n",
    "        return np.mean(image[::2,::2],axis=2).astype(np.uint8)[14:98,:]\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.state,reward,done,_ = self.env.step(action)\n",
    "        self.stack_states()\n",
    "        return self.stacked_frames,reward,done\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.stacked_frames\n",
    "    \n",
    "    def sample_env_action(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def render(self):\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# Let's create all the stuff now\n",
    "\n",
    "# Constants\n",
    "state_size = [84,80,4]\n",
    "action_size = 4\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.99\n",
    "mem_size = 100\n",
    "batch_size = 32\n",
    "num_episodes = 1000\n",
    "episode_length = 1000 # End an episode after these steps\n",
    "network_copy_frequency = 10000\n",
    "network_copy_frequency_step = 0\n",
    "\n",
    "# Variables\n",
    "processor = StateProcessor()\n",
    "memory = PER(mem_size)\n",
    "\n",
    "# Variables to save\n",
    "reward_holder = []\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "agentNetwork = DQNAgent(state_size,action_size,'Agent')\n",
    "targetNetwork = DQNAgent(state_size,action_size,'Target')\n",
    "\n",
    "# # Tensorboard visualiser\n",
    "# writer = tf.summary.FileWriter('/tmp/breakout/1')\n",
    "# writer.add_graph(sess.graph)\n",
    "\n",
    "def choose_action(state,epsilon,epsilon_decay):\n",
    "    '''\n",
    "    Choose epsilon greedy action\n",
    "    '''\n",
    "    rand = np.random.rand(1)[0]\n",
    "    \n",
    "    if rand < epsilon:\n",
    "        # Select a random action\n",
    "        action = random.randint(0,action_size - 1)\n",
    "        epsilon*=epsilon_decay\n",
    "    else:\n",
    "        # Select a = argmax_a(Q[s,a])\n",
    "        Q = sess.run([agentNetwork.Q_all_a],feed_dict={agentNetwork.state:state.reshape((1,*state_size))})\n",
    "        action = np.argmax(Q)\n",
    "    \n",
    "    return action,epsilon\n",
    "\n",
    "# Fill our memory with dummy values\n",
    "for i in range(mem_size):\n",
    "        # print(i)\n",
    "        state = processor.get_state()\n",
    "        action = processor.sample_env_action()\n",
    "        next_state,reward,done = processor.step(action) # take a random action\n",
    "        if done:\n",
    "            processor.reset()\n",
    "        memory.store((state,action,reward,next_state,done))  \n",
    "\n",
    "# This function helps us to copy one set of variables to another\n",
    "# In our case we use it when we want to copy the parameters of DQN to Target_network\n",
    "# Thanks of the very good implementation of Arthur Juliani https://github.com/awjuliani\n",
    "def update_target_graph():\n",
    "    \n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Agent\")\n",
    "    \n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Target\")\n",
    "    \n",
    "    op_holder = []\n",
    "    \n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "(32, 84, 80, 4)\n",
      "Episode : 0\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.0037748755\n",
      "(32, 84, 80, 4)\n",
      "Episode : 1\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.81870985\n",
      "(32, 84, 80, 4)\n",
      "Episode : 2\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.15333778\n",
      "(32, 84, 80, 4)\n",
      "Episode : 3\n",
      "Total Rewards : 1.0\n",
      "Loss : 56.06001\n",
      "(32, 84, 80, 4)\n",
      "Episode : 4\n",
      "Total Rewards : 1.0\n",
      "Loss : 0.0008460151\n",
      "(32, 84, 80, 4)\n",
      "Episode : 5\n",
      "Total Rewards : 1.0\n",
      "Loss : 0.0007177124\n",
      "(32, 84, 80, 4)\n",
      "Episode : 6\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.000598622\n",
      "Model Updated\n",
      "(32, 84, 80, 4)\n",
      "Episode : 7\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.000496122\n",
      "(32, 84, 80, 4)\n",
      "Episode : 8\n",
      "Total Rewards : 1.0\n",
      "Loss : 0.00041567543\n",
      "(32, 84, 80, 4)\n",
      "Episode : 9\n",
      "Total Rewards : 1.0\n",
      "Loss : 0.00033912036\n",
      "(32, 84, 80, 4)\n",
      "Episode : 10\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.42863977\n",
      "(32, 84, 80, 4)\n",
      "Episode : 11\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.00055429013\n",
      "(32, 84, 80, 4)\n",
      "Episode : 12\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.00043689067\n",
      "(32, 84, 80, 4)\n",
      "Episode : 13\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.0003397699\n",
      "(32, 84, 80, 4)\n",
      "Episode : 14\n",
      "Total Rewards : 1.0\n",
      "Loss : 0.00027368756\n",
      "(32, 84, 80, 4)\n",
      "Episode : 15\n",
      "Total Rewards : 1.0\n",
      "Loss : 0.0002082569\n",
      "(32, 84, 80, 4)\n",
      "Episode : 16\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.00014646725\n",
      "(32, 84, 80, 4)\n",
      "Episode : 17\n",
      "Total Rewards : 0.0\n",
      "Loss : 0.12590194\n",
      "Model Updated\n",
      "(32, 84, 80, 4)\n",
      "Episode : 18\n",
      "Total Rewards : 1.0\n",
      "Loss : 6.4686613\n",
      "(32, 84, 80, 4)\n",
      "Episode : 19\n",
      "Total Rewards : 1.0\n",
      "Loss : 0.00041079978\n",
      "(32, 84, 80, 4)\n",
      "Episode : 20\n",
      "Total Rewards : 1.0\n",
      "Loss : 39.452305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-a3e6067f7f59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnetwork_copy_frequency_step\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-16276567d0b0>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(state, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Select a = argmax_a(Q[s,a])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magentNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_all_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0magentNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1085\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    443\u001b[0m       \u001b[0;31m# Remember the fetch if it is for a tensor handle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       if (isinstance(fetch, ops.Tensor) and\n\u001b[0;32m--> 445\u001b[0;31m           (fetch.op.type == 'GetSessionHandle' or\n\u001b[0m\u001b[1;32m    446\u001b[0m            fetch.op.type == 'GetSessionHandleV2')):\n\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2103\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m     \u001b[0;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "print('Done')\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = processor.reset()\n",
    "    \n",
    "    total_rewards = 0.0\n",
    "    \n",
    "    # Collect experience\n",
    "    for _ in range(episode_length):\n",
    "        state = processor.get_state()\n",
    "        \n",
    "        network_copy_frequency_step+=1\n",
    "        \n",
    "        action,epsilon = choose_action(state,epsilon,epsilon_decay)\n",
    "        \n",
    "        next_state,reward,done = processor.step(action)\n",
    "        \n",
    "        if done:\n",
    "            memory.store((state,action,reward,np.zeros((state_size[0],state_size[1],state_size[2]))\\\n",
    "                          ,done)) # No next state is there \n",
    "            total_rewards+=reward\n",
    "            break\n",
    "        \n",
    "        total_rewards+=reward\n",
    "        \n",
    "        memory.store((state,action,reward,next_state,done)) \n",
    "    \n",
    "    # Train our agent\n",
    "    \n",
    "    # Sample a minibatch of transition tuples\n",
    "    batch_idx,min_batch,w = memory.sample_minibatch(batch_size)\n",
    "    \n",
    "    # Seperate out our mini-batch variables\n",
    "    states = np.array([min_batch[i][0] for i in range(len(min_batch))])\n",
    "    actions = np.array([min_batch[i][1] for i in range(len(min_batch))])\n",
    "    rewards = np.array([min_batch[i][2] for i in range(len(min_batch))]).reshape(-1)\n",
    "    next_states = np.array([min_batch[i][3] for i in range(len(min_batch))])\n",
    "    dones = np.array([min_batch[i][4] for i in range(len(min_batch))])\n",
    "    \n",
    "    print(next_states.shape)\n",
    "    \n",
    "    Qs_next_state,actions_next_state = sess.run([agentNetwork.Q_all_a,tf.argmax(agentNetwork.Q_all_a,axis=1)]\\\n",
    "                             ,feed_dict={agentNetwork.state:next_states})\n",
    "    \n",
    "    # Convert actions in current state of transition tuple to a one hot vector\n",
    "    actions_one_hot_batch = np.zeros((batch_size,action_size))\n",
    "    for i in range(batch_size):\n",
    "        actions_one_hot_batch[i,actions[i]] = 1\n",
    "    \n",
    "    # Convert actions in next state to a one hot vector\n",
    "    actions_ns_one_hot_batch = np.zeros((batch_size,action_size))\n",
    "    for i in range(batch_size):\n",
    "        actions_ns_one_hot_batch[i,actions_next_state[i]] = 1\n",
    "        \n",
    "    target_Q_next_state = sess.run([targetNetwork.Q],feed_dict={targetNetwork.state:next_states,\\\n",
    "                                                               targetNetwork.actions_:actions_ns_one_hot_batch})\n",
    "\n",
    "    target_Q = np.zeros(batch_size)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        if dones[i] is True:\n",
    "            target_Q[i] = rewards[i]\n",
    "        else:\n",
    "            target_Q[i] = rewards[i] + gamma*target_Q_next_state[0][i]\n",
    "    \n",
    "    # Make optimizer updates\n",
    "    _,loss,TD_errors = sess.run([agentNetwork.optimizer,agentNetwork.loss,agentNetwork.TD_error],\\\n",
    "                     feed_dict={agentNetwork.state:states,agentNetwork.actions_:actions_one_hot_batch\\\n",
    "                               ,agentNetwork.target_Q:target_Q,agentNetwork.weights_placeholder:w})\n",
    "    \n",
    "    # Update the priorites in the memory\n",
    "    memory.update(batch_idx,TD_errors)\n",
    "    \n",
    "    print('Episode : ' + str(episode) + '\\nTotal Rewards : ' + str(total_rewards) + '\\nLoss : ' + str(loss))\n",
    "    \n",
    "    if network_copy_frequency_step > network_copy_frequency:\n",
    "        network_copy_frequency_step = 0\n",
    "        update_op = update_target_graph()\n",
    "        sess.run(update_op)\n",
    "        print('Model Updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((state_size[0],state_size[1],state_size[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 80, 4)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
